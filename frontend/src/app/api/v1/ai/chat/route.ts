import { NextResponse } from 'next/server'

import { getDb } from '@/lib/db/sqlite'
import { prisma } from '@/lib/db/prisma'
import { pveFetch } from '@/lib/proxmox/client'
import { decryptSecret } from '@/lib/crypto/secret'

// R√©cup√©rer les param√®tres IA
function getAISettings() {
  try {
    const db = getDb()
    const stmt = db.prepare('SELECT value FROM settings WHERE key = ?')
    const row = stmt.get('ai') as { value: string } | undefined
    
    if (row?.value) {
      return JSON.parse(row.value)
    }
  } catch (e) {
    console.error('Failed to get AI settings:', e)
  }
  
  return {
    enabled: false,
    provider: 'ollama',
    ollamaUrl: 'http://localhost:11434',
    ollamaModel: 'mistral:7b'
  }
}

// R√©cup√©rer les connexions PVE via Prisma
async function getConnections() {
  try {
    const connections = await prisma.connection.findMany({
      where: { type: 'pve' },
      select: {
        id: true,
        name: true,
        type: true,
        baseUrl: true,
        apiTokenEnc: true,
        insecureTLS: true,
      }
    })

    return connections
  } catch (e) {
    console.error('Failed to get connections:', e)
    
return []
  }
}

// R√©cup√©rer les alertes actives via Prisma
async function getActiveAlerts() {
  try {
    const alerts = await prisma.alert.findMany({
      where: { status: 'active' },
      orderBy: { lastSeenAt: 'desc' },
      take: 10,
      select: {
        severity: true,
        message: true,
        entityName: true,
        entityType: true,
        metric: true,
        currentValue: true,
        threshold: true,
      }
    })

    
return alerts
  } catch (e) {
    console.error('Failed to get alerts:', e)
    
return []
  }
}

// R√©cup√©rer les donn√©es live de Proxmox
async function fetchProxmoxData(connections: any[]) {
  const allData: any = {
    clusters: [],
    nodes: [],
    vms: [],
    summary: {
      totalVMs: 0,
      runningVMs: 0,
      stoppedVMs: 0,
      totalNodes: 0,
      onlineNodes: 0
    }
  }

  for (const conn of connections) {
    try {
      // D√©crypter le token avec la fonction du projet
      const token = decryptSecret(conn.apiTokenEnc)

      // Utiliser pveFetch pour les requ√™tes Proxmox (g√®re HTTPS et certificats auto-sign√©s)
      const resources = await pveFetch<any[]>(
        {
          baseUrl: conn.baseUrl,
          apiToken: token,
          insecureDev: conn.insecureTLS
        },
        '/cluster/resources'
      )
      
      // Process nodes
      const nodes = resources.filter((r: any) => r.type === 'node')

      nodes.forEach((n: any) => {
        allData.nodes.push({
          name: n.node,
          status: n.status,
          cpu: n.cpu ? (n.cpu * 100).toFixed(1) : 0,
          mem: n.mem && n.maxmem ? ((n.mem / n.maxmem) * 100).toFixed(1) : 0,
          memUsed: n.mem ? (n.mem / 1024 / 1024 / 1024).toFixed(1) : 0,
          memTotal: n.maxmem ? (n.maxmem / 1024 / 1024 / 1024).toFixed(1) : 0,
          cluster: conn.name
        })
        allData.summary.totalNodes++
        if (n.status === 'online') allData.summary.onlineNodes++
      })
      
      // Process VMs (qemu) and Containers (lxc)
      const vms = resources.filter((r: any) => r.type === 'qemu' || r.type === 'lxc')

      vms.forEach((vm: any) => {
        allData.vms.push({
          vmid: vm.vmid,
          name: vm.name || `VM ${vm.vmid}`,
          type: vm.type === 'lxc' ? 'CT' : 'VM',
          status: vm.status,
          cpu: vm.cpu ? (vm.cpu * 100).toFixed(1) : 0,
          mem: vm.mem && vm.maxmem ? ((vm.mem / vm.maxmem) * 100).toFixed(1) : 0,
          memUsed: vm.mem ? (vm.mem / 1024 / 1024 / 1024).toFixed(2) : 0,
          memTotal: vm.maxmem ? (vm.maxmem / 1024 / 1024 / 1024).toFixed(2) : 0,
          node: vm.node,
          cluster: conn.name
        })
        allData.summary.totalVMs++
        if (vm.status === 'running') allData.summary.runningVMs++
        else allData.summary.stoppedVMs++
      })
      
      allData.clusters.push({
        name: conn.name,
        nodes: nodes.length,
        vms: vms.length
      })
    } catch (e) {
      console.error(`Failed to fetch data from ${conn.name}:`, e)
    }
  }
  
  return allData
}

// Construire le prompt syst√®me avec le contexte r√©el
async function buildSystemPrompt() {
  const connections = await getConnections()

  const alerts = await getActiveAlerts()
  const infraData = await fetchProxmoxData(connections)
  
  // Trier les VMs par CPU pour trouver les plus gourmandes
  const topCpuVMs = [...infraData.vms]
    .filter((vm: any) => vm.status === 'running')
    .sort((a: any, b: any) => parseFloat(b.cpu) - parseFloat(a.cpu))
    .slice(0, 10) // Top 10 CPU
  
  // Trier les VMs par RAM
  const topMemVMs = [...infraData.vms]
    .filter((vm: any) => vm.status === 'running')
    .sort((a: any, b: any) => parseFloat(b.mem) - parseFloat(a.mem))
    .slice(0, 10) // Top 10 RAM

  // VMs arr√™t√©es (toutes)
  const stoppedVMs = infraData.vms.filter((vm: any) => vm.status !== 'running')
  
  // VMs en cours d'ex√©cution
  const runningVMs = infraData.vms.filter((vm: any) => vm.status === 'running')
  
  let prompt = `Tu es l'assistant IA de ProxCenter, une plateforme de gestion d'infrastructure Proxmox.

IMPORTANT: Tu ne peux PAS ex√©cuter d'actions. Tu peux uniquement analyser et sugg√©rer. Si l'utilisateur demande une action, explique ce qu'il faudrait faire mais pr√©cise qu'il doit le faire manuellement via l'interface ProxCenter ou Proxmox.

=== √âTAT ACTUEL DE L'INFRASTRUCTURE (donn√©es en temps r√©el) ===

üìä R√©sum√© global:
- ${infraData.summary.totalVMs} VMs/CTs au total (${infraData.summary.runningVMs} en cours d'ex√©cution, ${infraData.summary.stoppedVMs} arr√™t√©es)
- ${infraData.summary.totalNodes} h√¥te(s) (${infraData.summary.onlineNodes} en ligne)
- ${infraData.clusters.length} cluster(s) configur√©(s): ${infraData.clusters.map((c: any) => c.name).join(', ') || 'aucun'}
`

  if (infraData.nodes.length > 0) {
    prompt += `
üñ•Ô∏è √âtat des h√¥tes Proxmox:
${infraData.nodes.map((n: any) => `- ${n.name} (${n.cluster}): ${n.status === 'online' ? '‚úÖ En ligne' : '‚ùå Hors ligne'} | CPU: ${n.cpu}% | RAM: ${n.mem}% (${n.memUsed}/${n.memTotal} GB)`).join('\n')}
`
  }

  if (topCpuVMs.length > 0) {
    prompt += `
üî• Top 10 VMs/CTs par utilisation CPU:
${topCpuVMs.map((vm: any, i: number) => `${i + 1}. ${vm.name} (${vm.type} ${vm.vmid}) sur ${vm.node} - CPU: ${vm.cpu}% | RAM: ${vm.mem}%`).join('\n')}
`
  }

  if (topMemVMs.length > 0) {
    prompt += `
üíæ Top 10 VMs/CTs par utilisation RAM:
${topMemVMs.map((vm: any, i: number) => `${i + 1}. ${vm.name} (${vm.type} ${vm.vmid}) sur ${vm.node} - RAM: ${vm.mem}% (${vm.memUsed}/${vm.memTotal} GB)`).join('\n')}
`
  }

  // Liste COMPL√àTE des VMs arr√™t√©es - group√©es par cluster pour r√©duire la taille
  let stoppedVMsSection = ''

  if (stoppedVMs.length > 0) {
    // Grouper par cluster
    const byCluster: Record<string, any[]> = {}

    stoppedVMs.forEach((vm: any) => {
      if (!byCluster[vm.cluster]) byCluster[vm.cluster] = []
      byCluster[vm.cluster].push(vm)
    })
    
    stoppedVMsSection = `
‚èπÔ∏è VMs/CTs ARR√äT√âES - LISTE COMPL√àTE (${stoppedVMs.length} total):
${Object.entries(byCluster).map(([cluster, vms]) => {
  return `
[${cluster}] (${vms.length} arr√™t√©es):
${vms.map((vm: any) => `  - ${vm.name} (${vm.type} ${vm.vmid}) sur ${vm.node}`).join('\n')}`
}).join('\n')}
`
  }

  if (alerts.length > 0) {
    prompt += `
‚ö†Ô∏è Alertes actives (${alerts.length}):
${alerts.map((a: any) => `- [${a.severity?.toUpperCase()}] ${a.message} (${a.entityName || a.entityType})`).join('\n')}
`
  } else {
    prompt += `
‚úÖ Aucune alerte active.
`
  }

  // Liste des VMs en cours d'ex√©cution (limit√© pour le contexte)
  if (runningVMs.length > 0) {
    const vmsToShow = runningVMs.slice(0, 20) // R√©duit √† 20

    prompt += `
üìã VMs/CTs en cours d'ex√©cution (${runningVMs.length} total${runningVMs.length > 20 ? ', 20 premi√®res affich√©es' : ''}):
${vmsToShow.map((vm: any) => `- ${vm.name} (${vm.type} ${vm.vmid}) sur ${vm.node} - CPU: ${vm.cpu}% | RAM: ${vm.mem}%`).join('\n')}
${runningVMs.length > 20 ? `\n... et ${runningVMs.length - 20} autres VMs/CTs en cours d'ex√©cution` : ''}
`
  }

  // IMPORTANT: VMs arr√™t√©es √† la FIN pour que Mistral les voie bien
  prompt += stoppedVMsSection

  prompt += `
=== INSTRUCTIONS ===
- R√©ponds en fran√ßais de mani√®re concise
- Utilise UNIQUEMENT les donn√©es ci-dessus
- Cite les noms exacts des VMs et m√©triques
- Pour les actions, explique la proc√©dure mais pr√©cise que tu ne peux pas l'ex√©cuter
`

  return prompt
}

// POST /api/v1/ai/chat - Envoyer un message au LLM
export async function POST(request: Request) {
  try {
    const { messages } = await request.json()
    const settings = getAISettings()
    
    if (!settings.enabled) {
      return NextResponse.json({ 
        error: 'L\'assistant IA n\'est pas activ√©. Allez dans Param√®tres ‚Üí Intelligence Artificielle pour le configurer.' 
      }, { status: 400 })
    }
    
    const systemPrompt = await buildSystemPrompt()
    
    // Pour Ollama, on injecte le contexte dans le premier message utilisateur
    // car certains mod√®les ignorent le system prompt
    const lastUserMessage = messages[messages.length - 1]

    const contextualizedMessage = `${systemPrompt}

=== QUESTION DE L'UTILISATEUR ===
${lastUserMessage.content}

R√©ponds en utilisant UNIQUEMENT les donn√©es de l'infrastructure ci-dessus. Cite les noms exacts des VMs et leurs m√©triques.`

    if (settings.provider === 'ollama') {
      // Ollama API - contexte inject√© dans le message
      const ollamaMessages = [
        ...messages.slice(0, -1), // Messages pr√©c√©dents sans le dernier
        { role: 'user', content: contextualizedMessage }
      ]
      
      const response = await fetch(`${settings.ollamaUrl}/api/chat`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          model: settings.ollamaModel,
          messages: ollamaMessages,
          stream: false,
          options: {
            num_predict: 4096, // Permet des r√©ponses plus longues
            temperature: 0.3  // Plus d√©terministe pour les listes
          }
        })
      })
      
      if (!response.ok) {
        const text = await response.text()

        throw new Error(`Ollama error: ${text}`)
      }
      
      const json = await response.json()

      
return NextResponse.json({ 
        response: json.message?.content || json.response,
        provider: 'ollama',
        model: settings.ollamaModel
      })
      
    } else if (settings.provider === 'openai') {
      // OpenAI API
      const openaiMessages = [
        { role: 'system', content: systemPrompt },
        ...messages
      ]
      
      const response = await fetch('https://api.openai.com/v1/chat/completions', {
        method: 'POST',
        headers: { 
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${settings.openaiKey}`
        },
        body: JSON.stringify({
          model: settings.openaiModel,
          messages: openaiMessages,
          max_tokens: 1000
        })
      })
      
      if (!response.ok) {
        const json = await response.json().catch(() => ({}))

        throw new Error(json?.error?.message || `OpenAI error: ${response.status}`)
      }
      
      const json = await response.json()

      
return NextResponse.json({ 
        response: json.choices?.[0]?.message?.content,
        provider: 'openai',
        model: settings.openaiModel
      })
      
    } else if (settings.provider === 'anthropic') {
      // Anthropic API
      const anthropicMessages = messages.map((m: any) => ({
        role: m.role === 'assistant' ? 'assistant' : 'user',
        content: m.content
      }))
      
      const response = await fetch('https://api.anthropic.com/v1/messages', {
        method: 'POST',
        headers: { 
          'Content-Type': 'application/json',
          'x-api-key': settings.anthropicKey,
          'anthropic-version': '2023-06-01'
        },
        body: JSON.stringify({
          model: settings.anthropicModel,
          system: systemPrompt,
          messages: anthropicMessages,
          max_tokens: 1000
        })
      })
      
      if (!response.ok) {
        const json = await response.json().catch(() => ({}))

        throw new Error(json?.error?.message || `Anthropic error: ${response.status}`)
      }
      
      const json = await response.json()

      
return NextResponse.json({ 
        response: json.content?.[0]?.text,
        provider: 'anthropic',
        model: settings.anthropicModel
      })
      
    } else {
      throw new Error(`Provider inconnu: ${settings.provider}`)
    }
    
  } catch (e: any) {
    console.error('AI chat failed:', e)
    
return NextResponse.json({ error: e?.message || String(e) }, { status: 500 })
  }
}
